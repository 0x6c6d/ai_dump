{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521145e9-7066-4c9f-8642-d0a5862a6980",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Process (MDP)\n",
    "\n",
    "<ins>Notations / Vocabular</ins>\n",
    "- Agent\n",
    "    - performs actions based on the state of the task environment\n",
    "- State\n",
    "    - $S$ -> finite set of values where $s \\in S$\n",
    "    - all relevant information that describe in which situation the task environment is currently in\n",
    "- Actions\n",
    "    - $A$ -> finite set of values which depends on the current state $a \\in A(s)$\n",
    "    - the moves the agent can perform at every moment in time -> chosen by the state of task at that time\n",
    "- Reward\n",
    "    - $r \\in R \\in \\mathbb{R}$\n",
    "    - an agent gets a positive reward for performing a desired action & a negative reward for performing an undesired action\n",
    "- Probability function\n",
    "    - $p(s',r | s, a) = \\text{Prob}(S_{t+1}=s', R_{t+1}=r | S_t = s, A_t = a)$\n",
    "    - probabilities of next state & reward based on the current state & some action\n",
    "    - Markov Property -> the probabilities only depend on the current state & action and not on the history of states and actions that lead to that point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87049d9-8333-4a86-bfc4-512ab1794a8d",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The Markov decision process is a discrete-time stochastic control process.\n",
    "- discrete-time: time moves forward in finite intervals ($t \\in \\{1,2,3,\\dots\\}$)\n",
    "- stochastic: future states depend only partially on the actions taken\n",
    "- control process: based on descision making to reach a target state\n",
    "\n",
    "![MDP](img/mdp/mdp.png)\n",
    "\n",
    "At the start of the task the agent observes the state $S_t$ of the environment and takes an action $A_t$ based on this state. This action causes an effect on the environment and transitions the environment from $S_t$ to $S_{t+1}$. Based on the change in the environment the agents receives a reward which gives feedback how well his action was. Based on the new state $S_{t+1}$ the agent takes a new action and the cycle repeats itself until the tasks ends.\n",
    "\n",
    "<ins>Trajectory & Episode</ins>\n",
    "- trajectory are the elements created when an agent moves from one state to another\n",
    "- episode is the trajectory from the initial state to a terminal state\n",
    "\n",
    "![Trajectory & Episode](img/mdp/trajectory_episode.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd09d06-7078-4df4-84f9-d5454b34cf36",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "$\\pi(a|s)$\n",
    "\n",
    "The policy is a function that receives a state and outputs the probabilities for all possible actions the agent can take. The agent then selects one of these actions, with higher probability actions being more likely to be chosen than those with lower probabilities.\n",
    "- Deterministic: the same state always returns the same action ($a = \\pi(s)$)\n",
    "- Stochastic: a state returns a list of possibilities for the next action ($\\pi(s) = [p(a_1),p(a_2),\\dots,p(a_n)]$)\n",
    "\n",
    "The goal is two find a policy that accumulates the maximum reward possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35a59a-a533-4c81-b160-c747e2883627",
   "metadata": {},
   "source": [
    "## Rewards & Return\n",
    "Reward: $R_t$<br>\n",
    "Return: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T-t-1} R_T = \\sum_{k=0}^T \\gamma^{k} R_{t+k+1}$<br>\n",
    "- episodic: T marks the terminal state (finite MDP)\n",
    "- continuing: if T would be substituted with $\\inf$ it would be a continues (infinite) MDP\n",
    "\n",
    "We want to maximize the sum of long-term rewards because short-term rewards can worsen long-term results. We can do this by introducing a discount factor $\\gamma$ which determines how much the agent values future rewards compared to immediate rewards. If done correctly this balances out short-term and long-term rewards.\n",
    "\n",
    "$\\gamma \\in [0,1]$\n",
    "\n",
    "When $\\gamma = 0$ the agent only considers immediate rewards.\n",
    "When $\\gamma$ is close to 1 the agent values future rewards almost as much as the immediate one.\n",
    "\n",
    "$\\text{max} \\ \\mathbb{E}_{\\pi}[G_t]$\n",
    "\n",
    "The goal is to select the policy $\\pi*$ that maximizes the expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91111ee-8564-45c4-ab2f-34762049cb72",
   "metadata": {},
   "source": [
    "## State Value Function\n",
    "\n",
    "$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s]$\n",
    "\n",
    "The state value function returns the expected cumulative reward the agent can obtain starting at state $s$ and following the policy $\\pi$.\n",
    "\n",
    "## Action Value Function\n",
    "\n",
    "$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t=s, A_t=a]$\n",
    "\n",
    "The action value functions returns the expected cumulative reward the agent can obtain at state $s$ when doing the action $a$ and then following the policy $\\pi$ thereafter.\n",
    "\n",
    "## Functions of optimal policy $\\pi*$\n",
    "\n",
    "$v_{\\pi*}(s) \\geq v_{\\pi}(s) \\quad$ for all $s$ and for any $\\pi$<br>\n",
    "$q_{\\pi*}(s,a) \\geq q_{\\pi}(s,a) \\quad$ for all $s,a$ and for any $\\pi$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb2e75-80fe-4daf-8f40-ed4c960a800e",
   "metadata": {},
   "source": [
    "## Maze environment\n",
    "\n",
    "The goal is to create an agent that finds the fastest way through a maze. The maze has following conditions\n",
    "\n",
    "- it has 25 states (5x5 grid)\n",
    "- transition between states are deterministic $p(s',r | s, a)$\n",
    "- all rewards are the same (-1) until the episode ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219d31ee-8255-4c4d-b252-196eabb076dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Optional, Iterable\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import animation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4697ea-ec31-4f9c-aadb-622f478970ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://www.udemy.com/course/beginner-master-rl-1/\n",
    "\n",
    "## class for setting up the tasks environment\n",
    "\n",
    "class Maze(gym.Env):\n",
    "\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        self.maze = self._create_maze(size=size)\n",
    "        self.distances = self._compute_distances(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self._get_next_state(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
    "                        # adjacent squares that are not connected).\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
    "\n",
    "        # Add the geometry of the goal square to the viewer.\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer.\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
    "        agent_col = int(scale * (self.state[1] + .5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        if self.shaped_rewards:\n",
    "            return - (self.distances[next_state] / self.distances.max())\n",
    "        return - float(state != self.goal)\n",
    "\n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Action value not supported:\", action)\n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "                for row in range(size) for col in range(size)}\n",
    "\n",
    "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
    "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
    "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
    "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_distances(goal: Tuple[int, int],\n",
    "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        return distances\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    plt.close(\"all\")\n",
    "    matplotlib.use('Agg')\n",
    "    \n",
    "    # Set up the figure and axis\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    \n",
    "    # Display the first frame\n",
    "    im = ax.imshow(frames[0])\n",
    "\n",
    "    # Update function for the animation\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "\n",
    "    # Create the animation\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig=fig, func=update, frames=frames,\n",
    "        interval=50, blit=True, repeat=False\n",
    "    )\n",
    "    \n",
    "    # Convert animation to HTML5 video and return it\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "def render_image(frame, state):\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"State: {state}\")\n",
    "    plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e346c1-57cc-4969-b00e-93faef37b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New episode starts in state (0, 0)\n",
      "After moving down 1 row, the agent is in state: (1, 0)\n",
      "After moving down 1 row, we got a reward of: -1.0\n",
      "After moving down 1 row, the task is not finished\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVlElEQVR4nO3deYydBd3o8d+Z9UxnyrQUsMBrN1DKchVZLqEWkKXUKwSvF0pKghYUkODbYq4GkDcGyRWwfUMJwhVFk4KF3CjGBV/tBapFkJLIYmuVl7yGRYHL1tJ91s489w/s7+0wQLczc860n0/SZPKc55zzm+bMfOdZznNKRVEUAQARUVftAQCoHaIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAO+iKK66IGTNmVHuM9N3vfjcmTJgQ3d3d1R6FPYgoUDGrVq2K8847LyZOnBjlcjkOPvjgmDFjRtx2220D1rvxxhvj5z//+S4/zzPPPBPf+MY34sUXX9y9gXfCCy+8ED/4wQ/i2muvHbD8jjvuiFmzZsWECROiVCrFRRddVJHnW758eUyfPj1GjRoV48ePj3nz5sWmTZsGrHPRRRdFT09PfO9736vIc0KEKFAhy5cvj+OOOy5WrlwZl156adx+++1xySWXRF1dXdx6660D1q1EFK6//vphjcKtt94akydPjlNPPXXA8vnz58dvf/vbOPLII6OhoaEiz7VixYo4/fTTo6OjIxYuXBiXXHJJ3HnnnTFr1qwB65XL5ZgzZ04sXLgwXMKMSqnMq5i93g033BDt7e3xxBNPxJgxYwbc9sYbb1RnqArp7e2Ne++9Ny6//PJBt/3ud7/LrYS2traKPN+1114bY8eOjYcffjj22WefiIiYNGlSXHrppfHggw/GmWeemeuef/75sWDBgli2bFmcdtppFXl+9m62FKiI5557Lo488shBQYiIOOCAA/LrUqkUmzdvjrvvvjtKpdKAXS5/+9vf4oorrojDDjssWlpaYty4cTFr1qwBWwR33XVX/sV86qmn5mM8/PDDuc6SJUvipJNOitbW1hg9enScddZZ8Ze//GXATL29vfHss8/Gq6++ut3v7fe//32sXr06zjjjjEG3TZw4MUql0nYfY0dt2LAhHnroobjwwgszCBERn/vc56KtrS1+/OMfD1j/2GOPjX333Td+8YtfVGwG9m6iQEVMnDgxnnrqqfjzn//8vustXrw4mpub46STTorFixfH4sWL44tf/GJERDzxxBOxfPnymD17dnz729+Oyy+/PH7zm9/EJz7xiejo6IiIiJNPPjnmzZsXEW//Rb31MQ4//PB8/LPOOiva2tpi/vz58fWvfz2eeeaZmD59+oC4vPLKK3H44YfH1772te1+b8uXL49SqRQf+9jHduW/ZqesWrUqtmzZEscdd9yA5U1NTXH00UfHH//4x0H3OeaYY+Kxxx4b8tnYSxRQAQ8++GBRX19f1NfXFyeeeGJx1VVXFQ888EDR09MzaN3W1tZizpw5g5Z3dHQMWvb4448XEVH88Ic/zGX33XdfERHFsmXLBqy7cePGYsyYMcWll146YPlrr71WtLe3D1j+wgsvFBHxrnO804UXXliMGzduu+u91/e1M7Z+b4888sig22bNmlWMHz9+0PLLLrusaGlp2a3nha1sKVARM2bMiMcffzzOOeecWLlyZSxYsCBmzpwZBx98cNx///079BgtLS35dW9vb6xZsyYOPfTQGDNmTDz99NPbvf9DDz0U69atiwsuuCBWr16d/+rr6+OEE06IZcuW5bqTJk2Koijirrvu2u7jrlmzJsaOHbtD38Pu6uzsjIiI5ubmQbeVy+W8fVtjx46Nzs7O3JqC3eFAMxVz/PHHx09/+tPo6emJlStXxs9+9rO45ZZb4rzzzosVK1bEEUcc8b737+zsjJtuuikWLVoUr7zyyoAzatavX7/d5//rX/8aEfGeB1y33Ue/s4phOrtnaxjf7b0HXV1dA8K51dbZKnlsg72XKFBxTU1Ncfzxx8fxxx8fH/7wh+Piiy+O++67L6677rr3vd/cuXNj0aJF8eUvfzlOPPHEaG9vj1KpFLNnz47+/v7tPu/WdRYvXhzjx48fdPuunjI6bty4WLt27S7dd2cdeOCBERHvegD81VdfjYMOOmjQ8rVr18aoUaPeNRiws0SBIbX1gOm2v+Te6y/an/zkJzFnzpy4+eabc1lXV1esW7duwHrvdf9DDjkkIt4+2+ndzhTaVVOnTo1777031q9fH+3t7RV73Hdz1FFHRUNDQzz55JNx/vnn5/Kenp5YsWLFgGVbvfDCC3mgHXaXYwpUxLJly951F8uvf/3riIg47LDDcllra+ugX/QREfX19YMe47bbbou+vr4By1pbWyMiBj3GzJkzY5999okbb7wxent7Bz3+m2++mV/vzCmpJ554YhRFEU899dR2191d7e3tccYZZ8Q999wTGzduzOWLFy+OTZs2DXoDW0TE008/HdOmTRvy2dg72FKgIubOnRsdHR3xmc98JqZOnRo9PT2xfPny+NGPfhSTJk2Kiy++ONc99thjY+nSpbFw4cI46KCDYvLkyXHCCSfE2WefHYsXL4729vY44ogj4vHHH4+lS5fGuHHjBjzX0UcfHfX19TF//vxYv359NDc3x2mnnRYHHHBA3HHHHfHZz342jjnmmJg9e3bsv//+8fe//z1+9atfxcc//vG4/fbbI+I/T0mdM2fOdg82T58+PcaNGxdLly4ddLzil7/8ZaxcuTIi3g7Nn/70p/jmN78ZERHnnHNOfOQjH4mIiBdffDEmT568Q893ww03xLRp0+KUU06Jyy67LF5++eW4+eab48wzz4xPfvKTA9Z96qmn4q233opPf/rT7/uYsMOqeOYTe5AlS5YUn//854upU6cWbW1tRVNTU3HooYcWc+fOLV5//fUB6z777LPFySefXLS0tAw4LXTt2rXFxRdfXOy3335FW1tbMXPmzOLZZ58tJk6cOOhUz+9///vFlClTivr6+kGnpy5btqyYOXNm0d7eXpTL5eKQQw4pLrroouLJJ5/MdXbmlNSiKIp58+YVhx566KDlc+bMKSLiXf8tWrQo11u1alUREcU111yzQ8/36KOPFtOmTSvK5XKx//77F1/60peKDRs2DFrv6quvLiZMmFD09/fv0OPC9pSKwkVTYHuef/75mDp1aixZsiROP/30nb7/d77znbjqqqviueeeiw984AMVmam7uzsmTZoU11xzTVx55ZUVeUxwTAF2wJQpU+ILX/hCfOtb39ql+y9btizmzZtXsSBERCxatCgaGxvf9ZpMsKtsKQCQbCkAkEQBgCQKACRRACCJAgBpp97R/NLLr8cVX5kfTlcCGDmOmDo5Flw/d4fW3akodHZ1xx+efmaXhgKgOnbmsup2HwGQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKmh2gOw92kdVY6f3rMgWkeVqz1Kzfu3Bx6Lb91yd7XHYC8iChUyZdJBceXlsyMi4rU33oqbFt5V3YFq1JRJB8VX514YR06dEo2NXn7b8+lP1UepVPJ6eg9+7irP7qMK2W/c2Jh97pkx+9wz41MzplV7nJq137ixce45pwnCDpoy6WCvp/fh567y/GRSNS+9/Hpc8ZX5UVR7kBp0xNTJseD6udUeg72QKFA1nV3d8Yenn6n2GDWpVCpVewT2UnYfAZBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkEXXtow/W18XourevCdNcKsUnyk3xQGd33v7Slv7YWLi8GsCuqtkoNEZEfSnigtaWGF//9gbN9HJjHNhQP2C9z45uya8f6+qJ/7elPyIiHursjj/3bomeIqJ/2KYGGNlqMgonlxvj/NZyHNnUEK2lUjTs4BUjP15uyq/PHtUcPVHE/9nUFUs7e+L5LX1DNS7AHqNmotBSipjSUB//vM+o+GhzY5R389LBLXWlaIlSXL7PqDh7VHM82tUT92zqilf7bDcAvJeaONDcEBHXtLfG3fu3x3+tQBDe6Z8a6mN2azl+fMCYOLncWNHHBtiTVD0KRzU2xL+MaY1PjWqOulJpyD5cpFQqRWtdKb45ti3OGdUcB9RV/VsHqDlV231UiohTyo1x3di2aB/GX9BtdXXxjbFtsbK7N77y1sZ4q9/ZSgBbVe3P5VPKjXHD2NHDGoRtfaSpIW7ed7QtBoBtVOU34lGNDXHd2LZoqave59CWSqX4aHNj/K9928Kn4QK8bdij0BAR57Y2V20L4Z0Ob6yPkxx8BoiIYY5CSyni62Na4+xRzcP5tO+rra4ubhw7Ok7d5j0OAHurYY3ClIb6OGtUc9QP0RlGu2pUXSkubCtHubbGAhh2wxqFf95nVM3uvz+6qSFOsbUA7OWGLQonlxvjo82NQ/Y+hN1VKpXistGjQhaAvdmwRKExIs5vLVf8ncqVdnBDXfy3GjreATDchiUK9aWII5tq5jJL76mpVIrDG2t/ToChMixRuKC1JVprfCthq9NbmmJSQ22cLgsw3Iblt9/4+rodvvx1tY2rr6v53VwAQ8WfxACkIY/CB+vrYvoIe8fwBa3lao8AUBVDHoXRdaVBH6FZ6z7kYDOwl7L7CIAkCgAkUQAgiQIAacijUEREfzGyPvKyv9oDAFTJkEfhr7198evOnqF+moq6dcPmao8AUBVDHoUtEdHZP7K2FDaOsHkBKmXYjikUI2QX0kiZE2AoDEsUfripMzaPkF+2D3T2xAu9fdUeA6AqhiUKq/v7o3MENKEoiljd3x/d1R4EoEqGJQrdRcQdGzqG46l2y4aiiB9s6Kz2GABVM2zHFH7X1RP/3rNluJ5ul9y7qSs2jpDdXABDYdiisLa/iPs7umv2QO5LW/rit509UZvTAQyPYX1H8286u+PvfbX31rCiKOKRrp54fosDzMDebVijsLq/iK+u2Rgv1dAv376iiF90dMf/HgHHPACG2rBf++i5LX0xf93mmtmN9Gpff9y4bnN01cY4AFVVlQvirejpjZ92dFf9mkjr+/vjlvUdUduHvwGGT1Wi0FFE/Ou6zfFvVQxDR38R163dFMu6RtZ1mQCGUtU+d7InIm5YtznW9BdxarkpJjUOz0d2FkURj3X3xk82d8WjXb3D8pwAI0VVP0+hNyJu29ARX31reA4+9xVF/KyjO65+a2M80tXr9FOAd6iJD9l5fktf/M81G+OeTZ2xpSgqfhC6vyjib7198e0NHXHTus0j4pIbANVQtd1H7/Tclr64dX1H/GhTV1yxz6g4vrkx9qvf/WY907Ml7u/oit909sQal8QGeF81E4WIiL6IeKWvP/5l7ab4L40NcVBDXXyurSUO/EccWutK0Vgqvef9N/cX0fuPrYz/29kdK3u2xB+6e2OtGADskJqKwrZW9W6JVb0Rv+3syX1c/721HJMb3j4g3ViKOLKpIVZ0/+cJpfd3dMVz/7jsdW/4WE2AnVWzUdhq2/ODfrS5K7+ui4j96+vi9Rq8bAbASFUTB5p3RX+EIABU2IiNAgCVJwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKnmr5IKe6O+vr7YsHFzRERs7uis8jTsTUQBatAf//Qf8bGTLoyIiH4fEsUwEgWoQf39/bFpsy0Ehp9jCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFqqpUqvYEjGTvfPl4Pb27nflvaRiyKWA7Jk04MB5dcme1x2AEK5eb82uvp/e27f/T9ogCVdPU1BgfOmRCtcdgD+H1VBl2HwGQbClQNS+9/Hpc8ZX5UVR7ENjDHTF1ciy4fu4OrSsKVE1nV3f84elnqj0G7PFKO3EE3u4jAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkURgipVK1J6hN/lugtjVUe4A90aQJB8ajS+6s9hg1qVxurvYIwPsQhQrp6+uLjZs6oq21JZqaGuNDh0yo9kg1rad3S2zu6Kz2GDVryqSD4srLZ0dExGtvvBU3LbyrugOx17D7qEL++Kf/iOmfvNQvuh308189HP/jwqurPUbN2m/c2Jh97pkx+9wz41MzplV7HPYithQqpL+/P958c23MmnNt1Ndr7fasXrMuOjq7qj0G8A6iUEF9/f3x9Mpnqz0GwC7zJy0ASRQASHYfAXuspukfirrR5WqPMex6//3V6Htx9S7dVxSAPVb5rI9Ewz/tW+0xht3mex/f5SjYfQRAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgNRQ7QEAhkr/a+tjS3+1pxh+xcauXb6vKAB7rE23Lq32CCOO3UcAJFEAIO10FEqloRgD2NY7f8z83LE7dublUyqKotjRlXt6euNvL726CyMBO6Ncbo4PHvyBiPBzx+7b9vW0PTsVBQD2bI4pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYD0/wEl8gCPKRznkAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# general overview how to use the Maze class\n",
    "\n",
    "env = Maze()\n",
    "\n",
    "# starting the first episode\n",
    "init_state = env.reset()\n",
    "print(f\"New episode starts in state {init_state}\")\n",
    "\n",
    "# now the agent performs an action\n",
    "# 0 = up, 1 = right, 2 = down, 3 = left\n",
    "action = 2\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"After moving down 1 row, the agent is in state: {next_state}\")\n",
    "print(f\"After moving down 1 row, we got a reward of: {reward}\")\n",
    "print(\"After moving down 1 row, the task is\", \"\" if done else \"not\", \"finished\")\n",
    "\n",
    "render_image(env.render(mode=\"rgb_array\"), next_state)\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa9e58-5656-42af-8f11-6d77d0db0c0c",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "The maze consists of a 5x5 grid so $S$ contains 25 different states.\n",
    "\n",
    "$s = (row, column) \\quad \\text{row, column} \\in \\{0,1,2,3,4\\}$<br>\n",
    "$S = \\{(0,0),(0,1),(0,2),\\dots\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4d8ea1-6263-45fc-b827-1a254d906168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial state is: (0, 0)\n",
      "The space state is of type: MultiDiscrete([5 5])\n"
     ]
    }
   ],
   "source": [
    "env = Maze()\n",
    "print(f\"The initial state is: {env.reset()}\")\n",
    "print(f\"The space state is of type: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413bdc56-9bf5-42b2-82f9-567157a9a9f9",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "The agent can take four different actions: $a \\in \\{0,1,2,3\\}$\n",
    "\n",
    "- 0 = move up\n",
    "- 1 = move right\n",
    "- 2 = move down\n",
    "- 3 = move left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54bdd7d-b10a-46e9-a79d-36d1ad3fb160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a valid action is: 0\n",
      "The action state is of type: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"An example of a valid action is: {env.action_space.sample()}\")\n",
    "print(f\"The action state is of type: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31439f2a-54f8-4ff2-b818-c25cd632de8d",
   "metadata": {},
   "source": [
    "### Trajectory\n",
    "\n",
    "$\\tau = S_0,A_0,R_1,S_1,A_1,R_2,S_2,\\dots, R_N, S_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6233a155-f1af-42c1-a812-fb4a9f65d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trajectory of length 3 looks like this:\n",
      "\n",
      "[(0, 0), 1, -1.0, False, (0, 1)]\n",
      "[(0, 1), 1, -1.0, False, (0, 2)]\n",
      "[(0, 2), 0, -1.0, False, (0, 2)]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "trajectory = []\n",
    "trajectory_length = 3\n",
    "\n",
    "for _ in range(trajectory_length):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    trajectory.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"The trajectory of length {trajectory_length} looks like this:\\n\")\n",
    "for t in trajectory:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667903a-564f-4fc4-8571-4aff464d2a65",
   "metadata": {},
   "source": [
    "### Episode\n",
    "\n",
    "$\\tau = S_0,A_0,R_1,S_1,A_1,R_2,S_2,\\dots, R_T, S_T$\n",
    "\n",
    "An episode is a trajectory that goes from the initial state of the process to the final one where $T$ is the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1316c3d-47a7-42af-aea6-cc62f6caf31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode took 170 steps & looks like this:\n",
      "\n",
      "[(0, 0), 3, -1.0, False, (0, 0)]\n",
      "[(0, 0), 1, -1.0, False, (0, 1)]\n",
      "[(0, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 2, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 2, -1.0, False, (2, 1)]\n",
      "[(2, 1), 1, -1.0, False, (2, 1)]\n",
      "[(2, 1), 2, -1.0, False, (3, 1)]\n",
      "[(3, 1), 1, -1.0, False, (3, 1)]\n",
      "[(3, 1), 2, -1.0, False, (3, 1)]\n",
      "[(3, 1), 1, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 2, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 0, -1.0, False, (2, 1)]\n",
      "[(2, 1), 1, -1.0, False, (2, 1)]\n",
      "[(2, 1), 0, -1.0, False, (1, 1)]\n",
      "[(1, 1), 1, -1.0, False, (1, 1)]\n",
      "[(1, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 2, -1.0, False, (1, 1)]\n",
      "[(1, 1), 2, -1.0, False, (2, 1)]\n",
      "[(2, 1), 2, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 0, -1.0, False, (2, 1)]\n",
      "[(2, 1), 0, -1.0, False, (1, 1)]\n",
      "[(1, 1), 2, -1.0, False, (2, 1)]\n",
      "[(2, 1), 2, -1.0, False, (3, 1)]\n",
      "[(3, 1), 0, -1.0, False, (2, 1)]\n",
      "[(2, 1), 0, -1.0, False, (1, 1)]\n",
      "[(1, 1), 2, -1.0, False, (2, 1)]\n",
      "[(2, 1), 0, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 2, -1.0, False, (2, 1)]\n",
      "[(2, 1), 2, -1.0, False, (3, 1)]\n",
      "[(3, 1), 1, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 3, -1.0, False, (3, 1)]\n",
      "[(3, 1), 1, -1.0, False, (3, 1)]\n",
      "[(3, 1), 1, -1.0, False, (3, 1)]\n",
      "[(3, 1), 0, -1.0, False, (2, 1)]\n",
      "[(2, 1), 3, -1.0, False, (2, 1)]\n",
      "[(2, 1), 0, -1.0, False, (1, 1)]\n",
      "[(1, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 1, -1.0, False, (0, 2)]\n",
      "[(0, 2), 1, -1.0, False, (0, 3)]\n",
      "[(0, 3), 3, -1.0, False, (0, 2)]\n",
      "[(0, 2), 0, -1.0, False, (0, 2)]\n",
      "[(0, 2), 2, -1.0, False, (0, 2)]\n",
      "[(0, 2), 3, -1.0, False, (0, 1)]\n",
      "[(0, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 2, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 2, -1.0, False, (2, 1)]\n",
      "[(2, 1), 0, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 1, -1.0, False, (1, 1)]\n",
      "[(1, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 2, -1.0, False, (1, 1)]\n",
      "[(1, 1), 1, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 1, -1.0, False, (1, 1)]\n",
      "[(1, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 3, -1.0, False, (0, 0)]\n",
      "[(0, 0), 1, -1.0, False, (0, 1)]\n",
      "[(0, 1), 2, -1.0, False, (1, 1)]\n",
      "[(1, 1), 3, -1.0, False, (1, 1)]\n",
      "[(1, 1), 0, -1.0, False, (0, 1)]\n",
      "[(0, 1), 1, -1.0, False, (0, 2)]\n",
      "[(0, 2), 0, -1.0, False, (0, 2)]\n",
      "[(0, 2), 2, -1.0, False, (0, 2)]\n",
      "[(0, 2), 1, -1.0, False, (0, 3)]\n",
      "[(0, 3), 2, -1.0, False, (1, 3)]\n",
      "[(1, 3), 1, -1.0, False, (1, 3)]\n",
      "[(1, 3), 1, -1.0, False, (1, 3)]\n",
      "[(1, 3), 0, -1.0, False, (0, 3)]\n",
      "[(0, 3), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 3, -1.0, False, (0, 3)]\n",
      "[(0, 3), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 2, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 2, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 2, -1.0, False, (1, 4)]\n",
      "[(1, 4), 3, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 2, -1.0, False, (2, 4)]\n",
      "[(2, 4), 0, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 1, -1.0, False, (1, 4)]\n",
      "[(1, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 2, -1.0, False, (1, 4)]\n",
      "[(1, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 0, -1.0, False, (0, 4)]\n",
      "[(0, 4), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 3, -1.0, False, (0, 3)]\n",
      "[(0, 3), 1, -1.0, False, (0, 4)]\n",
      "[(0, 4), 3, -1.0, False, (0, 3)]\n",
      "[(0, 3), 3, -1.0, False, (0, 2)]\n",
      "[(0, 2), 3, -1.0, False, (0, 1)]\n",
      "[(0, 1), 1, -1.0, False, (0, 2)]\n",
      "[(0, 2), 1, -1.0, False, (0, 3)]\n",
      "[(0, 3), 3, -1.0, False, (0, 2)]\n",
      "[(0, 2), 2, -1.0, False, (0, 2)]\n",
      "[(0, 2), 0, -1.0, False, (0, 2)]\n",
      "[(0, 2), 0, -1.0, False, (0, 2)]\n",
      "[(0, 2), 3, -1.0, False, (0, 1)]\n",
      "[(0, 1), 3, -1.0, False, (0, 0)]\n",
      "[(0, 0), 0, -1.0, False, (0, 0)]\n",
      "[(0, 0), 0, -1.0, False, (0, 0)]\n",
      "[(0, 0), 3, -1.0, False, (0, 0)]\n",
      "[(0, 0), 0, -1.0, False, (0, 0)]\n",
      "[(0, 0), 0, -1.0, False, (0, 0)]\n",
      "[(0, 0), 0, -1.0, False, (0, 0)]\n",
      "[(0, 0), 2, -1.0, False, (1, 0)]\n",
      "[(1, 0), 1, -1.0, False, (1, 0)]\n",
      "[(1, 0), 2, -1.0, False, (2, 0)]\n",
      "[(2, 0), 2, -1.0, False, (3, 0)]\n",
      "[(3, 0), 1, -1.0, False, (3, 0)]\n",
      "[(3, 0), 1, -1.0, False, (3, 0)]\n",
      "[(3, 0), 2, -1.0, False, (4, 0)]\n",
      "[(4, 0), 3, -1.0, False, (4, 0)]\n",
      "[(4, 0), 3, -1.0, False, (4, 0)]\n",
      "[(4, 0), 3, -1.0, False, (4, 0)]\n",
      "[(4, 0), 1, -1.0, False, (4, 1)]\n",
      "[(4, 1), 2, -1.0, False, (4, 1)]\n",
      "[(4, 1), 2, -1.0, False, (4, 1)]\n",
      "[(4, 1), 2, -1.0, False, (4, 1)]\n",
      "[(4, 1), 2, -1.0, False, (4, 1)]\n",
      "[(4, 1), 1, -1.0, False, (4, 2)]\n",
      "[(4, 2), 2, -1.0, False, (4, 2)]\n",
      "[(4, 2), 2, -1.0, False, (4, 2)]\n",
      "[(4, 2), 1, -1.0, False, (4, 2)]\n",
      "[(4, 2), 1, -1.0, False, (4, 2)]\n",
      "[(4, 2), 2, -1.0, False, (4, 2)]\n",
      "[(4, 2), 2, -1.0, False, (4, 2)]\n",
      "[(4, 2), 2, -1.0, False, (4, 2)]\n",
      "[(4, 2), 0, -1.0, False, (3, 2)]\n",
      "[(3, 2), 1, -1.0, False, (3, 3)]\n",
      "[(3, 3), 1, -1.0, False, (3, 4)]\n",
      "[(3, 4), 1, -1.0, False, (3, 4)]\n",
      "[(3, 4), 1, -1.0, False, (3, 4)]\n",
      "[(3, 4), 2, -1.0, True, (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "episode = []\n",
    "done = False\n",
    "counter = 0\n",
    "\n",
    "while not done:\n",
    "    counter += 1\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    episode.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"The episode took {counter} steps & looks like this:\\n\")\n",
    "for e in episode:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5048e9-ffde-4bdc-8b25-5f68f81707e8",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "$r = (s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1194723b-fdca-488a-b6f3-5882caea4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We achieved a reward of -1.0 by taking action 2 in state (0, 0)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "_, reward, _, _ = env.step(action)\n",
    "\n",
    "print(f\"We achieved a reward of {reward} by taking action {action} in state {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57460b0-1546-45df-a513-d2d5bb176b49",
   "metadata": {},
   "source": [
    "### Return\n",
    "\n",
    "$G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\dots + \\gamma^{T-1} R_T$\n",
    "\n",
    "$G_0$ is the return to the beginnig of the episode.\n",
    "\n",
    "$\\gamma$ will be set to $0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "826e0f60-1276-43eb-975c-f9e7b20071d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 732 moves to find the exit, and each reward r(s,a)=-1, so the return amounts to -99.93617917037557\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "gamma = 0.99\n",
    "G_0 = 0\n",
    "t = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    G_0 += gamma ** t * reward\n",
    "    t += 1\n",
    "env.close()\n",
    "\n",
    "print(f\"It took {t} moves to find the exit, and each reward r(s,a)=-1, so the return amounts to {G_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9dbdb1-b9c8-43c1-8495-77df973d5e2f",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "$\\pi(a|s) \\in [0,1]$<br>\n",
    "\n",
    "Since we have 4 actions that can be taken (up, right, down, left) we start with the following policy: $[0.25, 0.25, 0.25, 0.25]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6edb2761-b0f1-4d19-a37a-91248aeeb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.array([0.25] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8dbda84-f06e-41de-8bd4-3262cb11be07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"500\" height=\"500\" controls autoplay>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAQxNtZGF0AAACrwYF//+r\n",
       "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2NCByMzEwOCAzMWUxOWY5IC0gSC4yNjQvTVBF\n",
       "Ry00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMyAtIGh0dHA6Ly93d3cudmlkZW9sYW4u\n",
       "b3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFs\n",
       "eXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVk\n",
       "X3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBk\n",
       "ZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTEy\n",
       "IGxvb2thaGVhZF90aHJlYWRzPTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50\n",
       "ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBi\n",
       "X3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29w\n",
       "PTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj0yMCBzY2VuZWN1dD00MCBpbnRyYV9y\n",
       "ZWZyZXNoPTAgcmNfbG9va2FoZWFkPTQwIHJjPWNyZiBtYnRyZWU9MSBjcmY9MjMuMCBxY29tcD0w\n",
       "LjYwIHFwbWluPTAgcXBtYXg9NjkgcXBzdGVwPTQgaXBfcmF0aW89MS40MCBhcT0xOjEuMDAAgAAA\n",
       "BgRliIQAn9lnQQTHEoEe/T1jMRSYbUUAvtTDsX6kX35fgSBmEqW59OC/Pa3jEepdsAAAAwAAAwBA\n",
       "pzomoB/4x4ZybNx15K60HxeaGvBy3fNv766p24vRVjtPCK2mMFuBG/2pD6H10S2hPRFBZjDvg1eM\n",
       "C87mwsHtWQ7p6jtSTHw+f9Ngm/SFW6nqG/y8n9+qzRYaNTGDXWCMxV11pmdcQLy9v63SijiZXWmf\n",
       "wfid75TPMVXgdGEA8grVy+8KrjujBQJ/pwXNBDBne98FjENoc+Fn931HZpumbXaKKS6KuDjurK1x\n",
       "Vg4VQForetM0G3NqesfSjXmNxzax4mzoPPN7AAB0ThKJlaItPfTfLEv4LoVJIUzh6inRR8EaQtNH\n",
       "YEkNMQBo6SaMisltFcmR95EIInxQNWpQeL7ClfarzVFrrjjq5uiyWTmB0hMPLR2/AcPjVmCt4yDc\n",
       "mrxQmyRvjZDedQyMOGK/rnaShRK4ldfNoaJu+YYyUvWN9RbU8ZggABOa7WGWWvZUYvHjWKuNLNW8\n",
       "mXtdRUerVX5XJ5NazTNt29rrEjBORQ6W/seYlQ7ZUf+I2ALDgK/qM7kHvCa9GhX3buI/K7EwTFGG\n",
       "VgdduozWhresy3LpATCQrIunY8q65GQj1eztHZrOAxwfyRXVnorg+K4JfrNbucj4aQo9znXsG/Ht\n",
       "sUG8i2OGg+Fv2in9zRpmz+fOUbWJQkL6XUkFNxb/eWNQ6QZq3kH7gU8DFJX3Ozsgdg/G9jnegslw\n",
       "DLuoNfmC+c90HAqwzhCsOiHj+ZRAa793V8deST0kRG4hJfcaBE+8JAVTLcR/PVGUHoY2OgvdE0+C\n",
       "fE3ZgnIfrfAmIk9++gAHWvuRDBjsx4QA+F6mTOMk/SQBgZDw9M2zoN5RTdcVkDvpwGpA218uAipl\n",
       "EUl7wLPHsbsqenYXyMWTQLEmY80WIh+43ztvUx0UWvNst90w0ijjZqfnk7koyXp3xJgyEu6E6rHU\n",
       "s1DktwW5lzfFgBH+2UZiSbV2d8S8n+HFznsjvEnB0vm6N/izKcppno6razjo8PtLGtKUWZ4h6onX\n",
       "ogkAVwlLoUtnSIrDrd7sX4TNz4xzYZbdLIZ5RfSCjRFvWbFCu+K8VtlpazaqsEplr1r+WAr/DSE6\n",
       "VqERfbE8gwFr/L/8BYXXpumKAPDugudv0lqRt6k4rvalOoAgqR6/bWKye5reYiqdyq/ptWB4D2vL\n",
       "8n9a3JISTymmhMynRDj19vAXw6JgmItbzSoCr95iMJJSVQgDfa4dA1D5zUYLT2n81v2k+nKGneuf\n",
       "iETvEkOs05tDet/mxdp0OjSlUNTvaKISADOO+Xb/z2NBLoe+lg0RORNqWm0hx3uTX5Z1hAXHcc2s\n",
       "Innl1X/fVdbv7rF6SLBPoZV3Gyxro+0eHvOb6LBTrCVMnCBIic3d+XE5iS/3Gx55HFazg9cXUejn\n",
       "lbtz8Tnbv89gqHdGzssuEdB7NS+NEAM9vPCjziS5VT75W7bm0mj+uEAAAoNmY8387BB3cQYACyTw\n",
       "8n5EwvxbJV9kW4f9hVIFV6HOIu36nq8VS3dGyQWbMm/oJwUxC6ZSt4tje9ank7S4rld5nEb4mTqH\n",
       "t4XsuhLaCQ5p19XUAjL2R6SKyo2hA/nVF4DatmoKU8KOAMQ9wIO0xZkiC4gqm4zR1eVX3nFDuWVo\n",
       "uwfukNGYijX3gzUxKkomkumUsq8/rveu+LEtdDxPzauVygGuaLWCJuKmbZf+uei2mZtxQf7dkHNN\n",
       "uyY/d3gTdLLv+IFlZvlCY7xll57hRifEK+3jniroL24o2O2Z0wruJZEwE89jQMm4BTiyMCFthlaO\n",
       "67maIfSPq/HSxz/vfFkAlH1umrW1QFBr90gAXM/rHP3BaE3XMnRHuGXqxMLYex9ilHZMVFDeMmU2\n",
       "tpQDni3xCDxdgHrIvUaPN5LopRewJYwnXLuFj61RsBbUvZO8OM8HCoHMwJK79hP3aa4Rd7IWoZq6\n",
       "NqcoAQRV/zBee6u74zvlnNJPlAdNosKSJalZCCMiVbQ0laFQQ4IfDaNwajpddaJ9IAZAMWa++2Fx\n",
       "aQSxAAAAGkGaJGxJ/wAAP31OQ3Sg508J5bSAZEA8mAWcAAAAEUGeQniEvwRo/QgC3H/gALyBAAAA\n",
       "DwGeYXRC3wAAAwEqC9GCTgAAAA8BnmNqQt8AAAMBKlPRgk8AAAF4QZpmSahBaJlMFPJ/AB8/ynRc\n",
       "l/ktcKVXi+3ZtA0ugrAAPccUNq1apKURJHfQcORp6fc6yPi3rC/xa531qkdrRRpFyxoKxjmIJ/40\n",
       "Xc9ZPGRD1jXkUIpvr3Wn4wOi3Qs3yyfTkQnM2v5AEm6yq42OcMdJl0+Wd/k0UsxqZ7/VUSLZp0rF\n",
       "y/jLYcJGEFplpFxqeiLJANQOztCue5LD4UZaQdyvVJoNR+02i6IgGv94ERxT0MHF95deLq5JYOcO\n",
       "WrDlbKj1b/Z6pzH5vIWKoKneveXP6I0FK3p0sCKNtAYiEaw1KerVXbm1Nywk3/Z13SUSoza2KRgX\n",
       "GjSpXF2V7ECyX9D3mEV1Q6FVwYE7WNIMSOFbVBTTIJSgzIqCjgSj3rB3BPzQc27UPZvCdWGIAsZu\n",
       "QgITiEjM1W/SWB0OOPBYCkvIyZ737gAA5BU+yYPqHfevGKnYLziL45xHhsc7HR9pKtCpG1OtzvFC\n",
       "jPt+61OElU4LSoq8YCUBgwAAACIBnoVqQt8/WI057PFhDaNv6einjd59z7vQOHtABaZ7gAtpAAAA\n",
       "NUGaiEnhClJlMFLJ/wAd1wXpeQAADzCe4Je49gA6+r+eRrVQ7VUF2iImYOerP0eNHTEGuBqxAAAA\n",
       "GwGep2pC3z6ACDaD4j3pZupvD1omVksKoAAK2AAAAE5BmqlJ4Q6JlMCT/wAd1wXpeSC9sNCP9zbt\n",
       "OvFi0cz9CMo/Kfd2pJeqjZ4xLs83OkvySpc+e3eTcROoIpwkxWPYJZ4WfdJBqAlLfedfA4IAAAAv\n",
       "QZrKSeEPJlMCTwAd1wXpeQAADzCe4JcRgBKGxdTUaMnkVdSWbrX2ASiXlUAAPWEAAABCQZrrSeEP\n",
       "JlMCTwAd2qOLRPNTfgkl+9lwPUoknaFsWb2AAzbaIsE7X55lHvNGXUUINVFqVVb9+h2dA+aiMat3\n",
       "s3xAAAABb0GbDUnhDyZTBRE8nwAAID/sJ8nZ/IHxa5DNCLgACSwa/XOuj3/EqMDlecEzfnu6+1xn\n",
       "2dCRfJYJXnmhh4Nto9cylQ2GVS1BOdtfm/fxx2LMaaDKMK1yAndUH0khr1KAMSWhEUnufv2wHImw\n",
       "P9KL94FX215zcFIMNGmBuwVlqEt8c9ccalvDsdcmwYSy+igQJbBjNqLiFs8QUmh4DWv2GzUKc/Vz\n",
       "yYe9WfR0M0vzrvoF2plm7pgNfxVWuZe/3Ny/gPvjHKjK1yVILl4hILovsrxXJ0+078fSdMt6rE+1\n",
       "eg6ILV75TKWpD0thZTj2MhkqqKst6bLlxQ2rfz44pzYDtZ0ruqk/D43iCIdNdQ7VqIf0Adk5sQf7\n",
       "J/bGSlCJn3OJ9a7TIzdGwYta14poPpD7qHydXMRDfaISe3AsmMLdZD8J96xEJCF96XPwKtNQyYDx\n",
       "DIuO82mGeiCQjP8o9ivhXB6ab9To8KpCHI8wh+nAFJAAAAAkAZ8sakLfAHUADgOsEuHN0e4DC/hW\n",
       "p1PTJ7E1sHRmEJN0CAIHAAAANUGbL0nhDyZTBTyfAAAfyGH6XkMvWIxKiocg4SUNCURAkamh4k3j\n",
       "AJULDcBbckO/pOH7gCkhAAAAHAGfTmpC3wB1FDIHcQHJWiskXRKjPKy9vYgAHzEAAAA/QZtQSeEP\n",
       "JlMCTwAAID/sJ8nZ/clwpXHt/rDwbHZeZtvlLFGdF1aI1XKBfj6A7ZKMzh+Sx0H1dkGCiBkpwBSQ\n",
       "AAABl0GbcUnhDyZTAk8AAAMAOe1Ry4S1ZjkAAnEFcJIi2tBamXEEA9dEbyTqd7rUF1u38YITc9m7\n",
       "CM0DUWRyPXu+BAvWebJCqxuZJJPfDBiB7pA0fpCF44bNmHItwqeSyIu8KkfteLjEAyk1wMPXjFY3\n",
       "EU32QtUdh87qj13L4Uyi08pOy2Sx2cJR9STYun71KftYF1x11XgayDN7T/1uPHSX2wlW4ZeOjpcE\n",
       "SZmzCvV0haIiR4XjJy03Lhqo/wiNfKm2GQZWG7izO20Q5b11YQE+EITrocycfwPaz8NC61RzJRDa\n",
       "WmHUbUo2Q3eMmwalUVfeEc8nENFLunvlXWpGemqJx4sXePYcf38rUZVtCjYCaaaQkw5tG6XBNKNE\n",
       "fdbRIl2KR4oled4KQzNu9LLDGscuW9Lp3SnHE3dlFrwXHXlQgqO2nQnQ2hchL68SGx1giDek3/kF\n",
       "+Qo5Hv29BxyjT8DhUBGW+TRTlA2XYczSD1ixBfUf90ZYT4JFtw/tXbXZIf44lbXR7H2FYZ3GzwBi\n",
       "FQnEmqkRcAR+/VQQAAABhEGblEnhDyZTAk8AAAMAAGq/y48xyfbbJ5Y5iSCKmsxFUEVYMJjXKj+L\n",
       "Gy6Aj5fToqnWUexwvKgYs+VIE+vmy9YigSHvQV/5YfiD5bEXH3f+caIv87xQe2YePqiPRfYFCtEw\n",
       "3M18JQoR0e6wGLuCRAlHpDV15A8g789GubUUuEeQ2qHMqedNclaGOP3vafV0EUdNMFbgE27oxHrv\n",
       "1XXXy3OWnyfFC1Uh6w1PFpfpkBzk2cjRhIgM2xXDnWY4N1FCcHVZRm1YgBDOslGOmGk5eOi0Q9uU\n",
       "djICqXnBZIDX+BO5xIBWgWavXdx6BAV1MwlE2e5N6Sdeed0jMu2oOYbuEmFphAs3tnXoTXW34Ce6\n",
       "sub7txTPld9RycHw/s5Au7HoTF2Eqq0JPELzxVq8tlDUyA1tKgTy2PKOX8EepWkuasdZ+JcBsuPD\n",
       "lcH8PM/PaD1CgCegMxkeB9GXWVj8Y1c2pYJ5yWYJ0bs3eUFBZJ1F533G2w2cxHdHe1NOCNDkdE5u\n",
       "PWcFnnUAAAArQZ+yRRE8Kf8AAFZ4t+0JSmSETAt1x5+Ny1gH1kfWudLs4DFU/R4p3IwG4AAAABkB\n",
       "n9NqQt8AAAMBfJKvzj8aZZMWcwh5oliwAAABekGb1UmoQWiZTAk/AAADAAADAFv/2E+SXT5+hVaq\n",
       "k4wZtfvMvsW/7rb06jr/xaJZKvHq/A35/HVEn4g68etF0cW9yzTFsvKVUgc4Gkt8yZRbCcql6nbI\n",
       "1Iflj43JV0VW0l9UHbqTVjpnbCoq0+6BptThUrFtw4+GXByqxg8G6pd0BXLDOHZ5cG9erTT+L+pQ\n",
       "OO6qBxnXd/Q08DR9JQszQLuB+m8N8b29O5WZ22rtYh5lSADbFOonwgUadWdM7cQ0AlN1AKFhtBGf\n",
       "433PtKt64AkXKCjNALZS+GVA4da+kpzHBXEI9e5ZaLZ1tGQXKHusOyPbsHYZLaK4NLCZLGvXhElJ\n",
       "IdkEOtoF6FR2+JBoPawSLgHqTxsgx2jrhdmjpGMo/pQ0KFWM1BTAIcLWW/3dMVJJhBkyDpoHJbFK\n",
       "gNp6GIjoccWTTlf5tPrCNWktttEXmrBBMD7qmI0F5Nmafsk8DFYq85E4l8Jcph1xhJNXLJZ8YcT9\n",
       "PRwDWhkqYQAAAD9Bm/dJ4QpSZTBREsn/AAADAAADAFrk59hrcTXf8Xr8nE1IWm0BvAEIRo/ca1HU\n",
       "KaEUi+j44BOYqjEjF2UUbmAAAAAkAZ4WakLfAAADALHWEQx9dmhwV2YAETJf8neN+Dj8QqkvR191\n",
       "AAAAT0GaGEnhDomUwJP/AAADAAAlOxs3A2LCe76nudx579dn21kAge743D4/vdam4Ln5+VD7btrH\n",
       "ppZuG0a1maDAhCBX11cqO5V2qAVbBTHy/2EAAAAzQZo8SeEPJlMCTwAAAwAAAwBa5MP00b/mXsWI\n",
       "YAUnMguAiaDDQQop4lTf+NAGR+tbJ4LSAAAAM0GeWkURPCX/AAADAJ+ZfgPSHW836mfkudigI+is\n",
       "AI6Fxnc3yhmR0aakwY3xkYASAf4mwQAAAB0Bnnl0Qt8AAAMAAtRag4RHYAIj+zXo70kOzV7/JwAA\n",
       "ABgBnntqQt8AAAMAAs0qdUKdrwCIijh/HC0AAABAQZp9SahBaJlMCT8AAAMAAAMAW//YT5JdPohA\n",
       "jG0oTYiGQApCAl6IKz75cxgZvr6FUbjXbWwDZXNLlXfWRvMIYQAAADdBmoFJ4QpSZTAk/wAAAwAA\n",
       "AwBa5MSNj1/9oEBNb+gGq2q0aFs2B0y1KRdLxkh6ymLIoAa7k7xpAAAAI0Gev0U0TCX/AAADAJ9e\n",
       "x2iFfPR/PSJwAw/CX5K128xlhCpwAAAAGwGe3nRC3wAAAwABSwAPUADdev3ALOH8huABawAAABMB\n",
       "nsBqQt8AAAMAAtceMbmaep2wAAABlUGaxUmoQWiZTAk/AAADAABqt839738YITc9lHRxi9cCMkuF\n",
       "GUSpvWebJCqxuZJJbVVqW67pA0fpDdAbmtaoIm5mk/o5d4VI/a8XGIBlJrhukFiP44iKb+MWq6pk\n",
       "I8g2rW8D16SKoqag2OqvKaUfUk2LTSWzttJ3loX/64HpPtXdx8f4rDD83mGlrn1uCJGMZVXq6QtE\n",
       "PU8Lxk5gcXiw5ljpcp/Qf2k+ysN3Fh1thXoz5qwbPIZO64TIfJfs5yRprXOQnx3y3bK3lNEp7/+w\n",
       "BQJZpsMDQPdWFKPUievep75RM0qfCWc1rxC11RcT5CHQoFe/H8FkxQJm/y67djB7pbtVJizpqmAP\n",
       "sW93TUHv9htvHkp7fzYWGLI5c0ZUXugP4tGgTP7PyRxHYA37qcBGPoiBPVU94kNjqm6UUkfPkF+Q\n",
       "zSiboYy4/xoZh7TCIhMt7sinKxMZMZzJ/xR3LaGQ6XKKxY0I6h/U7h3ZhwYOEmTY8GnoqtVyIWJh\n",
       "b8bPAGIyG+/3UYi4Aj+TsQlTw22/4zsA1ywUdwAAAEFBnuNFESwl/wAAAwFd0WgTvm9PDRFcKK7u\n",
       "SyPhFoK9ShXRpJ+Tt2PyI94E8c/4y5dYWnt9uJCKOet1SYObBL1H+AAAAB4BnwJ0Qt8AAAMBmvj2\n",
       "6xgtrBltXPMrGh0O/fs+7XsAAAAbAZ8EakLfAAADAZcI8g/3xSl3CtyPsRHrExEvAAAAOkGbBkmo\n",
       "QWyZTAk/AAADAABtP8p0ltHVXHIm8FfdDifgSoOr0HbN0pCRHE+m6LZUBujX1ONLDUXgYMEAAAA4\n",
       "QZsoSeEKUmUwUVLJ/wAAAwAAaZq/6I9ESmSBVInVudWDHNRHssRRfHkjvlaLqe0xcbUMlhxR6O0A\n",
       "AAAeAZ9HakLfAAADAYJXvAAn9eQQkf4/J+t9D/eFSxXgAAAANEGbSUnhDomUwJP/AAADAABqv8uQ\n",
       "YBG+oceHCjz69mtZPwxLoyid4g4gSHhqt/WXI2AcZ50AAAAxQZtqSeEPJlMCTwAAAwAAaYzOzeQB\n",
       "VQAAVAWwi0v5NAH70Z31Xv8SU813ySz3wLvMGQAAADNBm4tJ4Q8mUwJPAAADAABqv8uQYDXgEffQ\n",
       "Ms61/Kr8idckW50BfhgT4g0WlkMDzAKFR2AAAAF3QZusSeEPJlMCTwAAAwAAAwBb/9hPk7QIiP1a\n",
       "qk4wZtfvMvsW/7rb06jr/xaJZyPHq/A35/HVEn4g68etF0cW9yzTFsvKVUgc4Gkt8yZRbCcql6nW\n",
       "dNIflj43JV0VW0l9UHbqTVvMFbCoq0+6BptThUrFtw4+GXByqxg8Gyd+6g5YZw7POT3r1aafxf1K\n",
       "Bx3VQOM67v6GngaPpKFmaBdwP03hvje3p3KzO3HLrEPNsDtk7OMk+ECjTqzpnbiGgEpuoBQsNoIz\n",
       "/G+54ABS1e6DSaM0AtlL4ZUDh1r6SnMcFcQj17llryZ96iRa8e6w7I9uwdhktorg0sJksa9eERQb\n",
       "ar7jaYk9Co7fEg0HtYJFwD1J42QY7R1wuzR0jGUf0oaFCrGagpgEOFrLf7umKkkwgzp1oUDktilQ\n",
       "G09DER0OOLJpyv82n2j7TGK02iLzVggmB91TEaC8mzNP2SeBisVecicS+EuUw64wkmrlks+MOJ+n\n",
       "o4BrQyVMAAAAM0GbzUnhDyZTAk8AAAMAAAMAWuTD9NG/4sITu+JAFUUHGYsDmduxggKLRbWAHJ/E\n",
       "kkVUGQAAAYtBm+5J4Q8mUwJPAAADAABqt839738YITc9lHRxi9cCMkuFGUSpvWebJCqxuZJJbVVq\n",
       "W67pA0fpDdAbmtaoIm5mk/o5d4VI/a8XGIBlJrhukFiP44iKb+MWq6pkI8g2rW8D16SKoqag2Oqv\n",
       "KaUfUk2LTSWzttJ3loX/64HpPtXdx8cZN9dtvcq3DLxyJHgiRjGVV6ukLRD1PC8ZOYHF4sOZY4KG\n",
       "vlTSg6srDdxYdbYV6M+asGzyGTuuEyHyX7Ockaavjb/g5koe9CZzaf9KNkPlWr1i+l5b3hHPJxDQ\n",
       "Swto5RM0g5L2WiC8S7x7Dj+/lW4w8FkxQJm/y67djB7pbtVJisgtv6Es5T//FErzvBSGa2f4ssMW\n",
       "Ry5oyovdAfhZeMoteC468rgIWbidCPEtC43A2iSGx1TdKKSPnyC/IT22r7egv8/KeBcsgmXlTIpy\n",
       "sTGTGcyf8SAIX1H/dGWE936xUP6ncO/7D+skS10ex9b5EJKsa8YjIb7/dRiLgCP5Ow7ZfjZwADbA\n",
       "uTiPOwAAAZ5BmhBJ4Q8mUwURPJ8AAAMAOp4EcIVD4lRgcrzgmb893X2uM+zoSL5LBK880MPBttHr\n",
       "mUrnSixxxAnO2vzfv447FmNNBlGFa5ATurhBuXsG0QAxJaERSe6BkFmmBJdl+WpDZV+JY4kyhsK3\n",
       "RjA6eDZahLfHPXHGpbw7HXJsGEtXxeKRKUGM2ouIWzxhXxks+D6+NGcycxChoX9vdUFtDil756Bd\n",
       "qZZu6W5aVum2LL3+5uX8B98Y5UZWuSpBdSIdbQhcJmFcnT7Tvx5AsPcWIu9q9B0QWr5LNHth3c/1\n",
       "sSsx1iBDetKk5suW2d1b+fHFObAdrOlfshrvhbaCFZKWGDH+Qh9AHZObEH+yhjO6A+oQbnFP5UQX\n",
       "HsAZZMWt0vYdO9XpIZQqFQNTGLiqS8KRgFwFRuoalAflDcoaPjq3kdIbF6z0dLv6QITnxduvyvCf\n",
       "esRCQgsbEAk0ydxGQ+oy9kCE24Sq3DNcP5R7FfCuD31mtJtOZ/Ne3XFzG6Xi7HSkL5JwlU3R4Wse\n",
       "Ch8tNoVkXiNnjs3WBXQBl1gaS3clUEEAAAApAZ4vakLfAADcQdU7SG8T4oMGgq+NFpD0zunmkTYX\n",
       "/D4h8Nch2EsN5+AAAAA9QZoySeEPJlMFPJ8AAAMAO//lOi5L/IHxa4TMQrZyBTwzjJ9S6939H0tL\n",
       "ijEK9DrDtY1iF1oAQaMEnmJVBAAAACEBnlFqQt8AANoAEkevu2A4tAmLRUB3QW/7h07qZFcsPmEA\n",
       "AABDQZpTSeEPJlMCTwAAAwA56bDkKMrQXFPtUfSxVgb03SKDGlyRoflm+3tPhVNpCn91dAkv7iGQ\n",
       "NbFLME0rXgUST3qoIAAAAEdBmnVJ4Q8mUwURPJ8AAAMAOe1Ry4S1Nz+69XYDeVfG8Ttg0UUip0hJ\n",
       "ZxFNH34tSHElcSngHNGukGfWHaLH407Qzfq9Ud/sCAAAACEBnpRqQt8AAN0vAWx4CiiY1ZPtKWCf\n",
       "D2GwORO0Tp5POOEAAAGCQZqYSeEPJlMCTwAAAwAAbT/KdJbSEuM2t8psa/JJsIAwYab57f6qVCrs\n",
       "Ll9OiqdZR7HC8qBiz5UgT6+fk9iKBIe9BX/lh+IPlsRcfd/5xoi/zvFB7Zh4+qI9F9gUK0TEGrXw\n",
       "lChHR7rAYu4JECUekNXuiDyDvz0a5tRS4R5Daocyp501yVoY4/e9smkQijppgrcAm3dGI9d+q66+\n",
       "W5y0+T4oWqkPWGp4tRdPQKHnKa6MJEBm3PTY6zHBuooTg6rKM2rEAIZ1kox9s55CzudaXmwMWpAV\n",
       "S84LJDd9IJ3OJAK0CzV67uPQICupmEomz3Jxfq+3zukZl21BzDdwkwtMIFm9s6+FaFbfgJ7qy5vu\n",
       "3FM+V33KAgfD+zkC7sehMXYSqrQk8QvPFWry2UNY/8CwL7eiC8o5fwR6laS5qx1n4lwGy48OVwfw\n",
       "8z89oPi/la48a2Qwr0jzBzhppq5tS0DzkswTo3Zu8oKCyTqLzvuNths5iO6O9qacEaHI6Jzces4L\n",
       "POgAAAAuQZ62RRE8Kf8AANhdjTJA6npWa2QAv3FstF8ld6soX1n5cDleDS/P/aPFDUgLKQAAABkB\n",
       "ntdqQt8AANxKDhABNEYLZL1LJG6XLntVAAAAN0Ga2UmoQWiZTAk/AAADAABpmr/olA4H3w/DfSy/\n",
       "/ILei4KcgmEn+ZtWhwRJ9c3ldSjz1Tsbh50AAAA8QZr8SeEKUmUwJP8AAAMAAGq/y5BgEb6hx4cK\n",
       "PPtPy+EYvSdoBKUIEJPICAzoyd5A3FshdpK5jmtvo6ElAAAAJkGfGkU0TCn/AADYXY0yQOp6VrdX\n",
       "9wmvXLzL4XpC7ak9JLbT1rCMAAAAFgGfO2pC3wAA3EoOEXvcQkB/CetulJEAAAG7QZsgSahBaJlM\n",
       "CT8AAAMAAAMAW//YT5JdPqUwLia7mZKVAs4yM438kEM//9DUu0BuusFdLoX19sa3dZI6QaE0GCd9\n",
       "nBt6PjhnSR9Av0qkypaDwQuCJGtI0WEPP55wOzwDGt015PrfNHyUQaM4E7+KNJ1f05s59tsFPzrd\n",
       "/NpzEUfbymf2KnVh5Y+KDtjJgpHdHQc2HtrmwziJ0axfcGaDU//z9sd3mr1nqT7k4yAG86+YY1F3\n",
       "A/TdDT1HeSMoirRRjz1pi2qhHMSyAVZzU6k62aLFvJ8FhyEwmXS5qNbbcof7yUX5w0bJLy0J9xw8\n",
       "8V8tptnlPo2pFnSG2f2iSiYopBfhjsPqyviBAw1devB6c1aTrKEekWxSQr3ZmRJx/ipsLhD4TFWE\n",
       "oKEw30DQUKHcqrZqHqdNAf0M/S+oXlMAV6xBzQTVPrCleEoIYs1NLewc0Owm3MGyUpuNxOf+o0aO\n",
       "L/8YNPPDlpS2+kQ3KgUYawoKzJaNrzeZejOYoE2s0Jicjtotvo73+/+az4IH6WqylKdH8n0vb3QM\n",
       "8m2qMB9LjxISr5fBnZmonv4bgfiVpwsZ1RSHJogBEmlWrfEAAAAsQZ9eRREsJf8AAMkQvbQwjQF9\n",
       "5At9xfh0CWfToynxkb48j2nzOflpTyquVZIAAAAhAZ99dELfAADaP3ECYgA+u4E+GrI1M6Ve8pwF\n",
       "+o/BFkCTAAAAGQGff2pC3wAA3EoOEXvcGSZEoeqGNm0GzjkAAAGWQZtiSahBbJlMFEyfAAADAAAD\n",
       "AE5/2ExFD+FUHCdtkIh/x00BLJm0BwEwPsE226SIoxjkdMXH0P1A9VyFvUvRYm0PFdI2YvVkDjiK\n",
       "WFTeraUX0hWY6gV+oyDtJqfXHLDhUnF+oc0bIUDRAH6rQgnZBbGvC2RWoKd1SZT/XQxkDteJy/HT\n",
       "pz+ic+vax6/07uDpEJ2EvJOPctxkAbq1Dwi1NhCezDygO7aARpS8R73Ecc8eKFOwWzjSdNUCvAz1\n",
       "6QG9EPuj6tCPs8Lm3nN40UmDe4Tkop+RL2HwzpgIw62b0q08j8nOB7Z8bTd7LVsUHG6YmuwEr+cK\n",
       "UVb8vkJ8x16L+b0ajnpS02M18KqgIzb/ja4CL/7gBst35PS6jLiu3kCR2vu+SPhPnity+c3qeZQ6\n",
       "AKcaYNjl60/FJJhqquCh1gs70Hcj3diDZZKP4mNHJQgUhRTszWC8Y2coRT/J5tNA0VvojoUKfb3e\n",
       "d2R6+RqKLr1Dp1JrMtB+53I+GTtco8vLcNnC/AWB80Mdyu7h0VfN+CGz7AMygAAAACYBn4FqQt8A\n",
       "AN035MurKpR1uCzQqcTfcxOS028GEAC/R0MdhJiLSQAAAaRBm4NJ4QpSZTAk/wAAAwAATnljURdv\n",
       "4wQm57KPmDRLPoeS+19dtm1flcxJPgLtmn4CqPfvY57M53qJKk8Uziukb3n0+jzsv3yrLQebPqKf\n",
       "0h762jPxTOoflveqUg/vOFkrAD0DQlpBTdXXEMqCeZXJMq/R/B/Y+pSF8cl1ao4xgVF0GW9fd6u6\n",
       "YICALyw2DV2iiKWGcycILNdz+mHNFy5sY8K4GqoMWycOZAYzH6IZgvshz98Lp3NRO8U87iFZfzoj\n",
       "WmOEEk0KOj6g1fnnOt/SEHSyjsHu9kr7qrMjDszsxl7rg5fsT98I+gEMQCp8m/RpaeGrEuSBb/m6\n",
       "+ng1OGFVM2y0/PEVueB93FCVGL8mf0WDJIBsgUJQHQQFBGRag4HEME6rzM2517XF2x6dMDqOtniM\n",
       "jB+51e64aFHlJT5GpPyjyZYM3U0TYmAjfOaS1ohyVHcTEWyKddVoFJ4m9vuW+Kh2Pj9rUkzmcSn7\n",
       "4uoQunbVn9TUFuO/RkA4JYxNERNRQP+lbnsDX4WPTtA/9zV28YlTyx5MtcG0af6+WVIChAvPXjAA\n",
       "AAGjQZukSeEOiZTAk/8AAAMAAE2TBO+jTdu/jBCbnspHF7KlzOGojojX12mnnspiSTlLWun0moNF\n",
       "c/yuNGLGOQuobKqPtEfyhuL4vV3Ud95NKA1FdmDOQLH8huzm1CM2jJQ0Zw8cgIChOZDHHx/7iP5j\n",
       "+JxkxV3D4xwd0B+gy4GnJPD4PHCPyYJPlei5V43c+fEU54axLz0PRhMSueWedXW9HdwgMxPeP7cg\n",
       "zQ7f4Nj7+4OcRJ+siEDfqLcz9P0Pg8xsA7EBlrflJQZhnFcHfmluyJqmYvqGxwZTBjIjhSTWkjjx\n",
       "5Fw1sDQiRrGxrpRcg4y3uBz9+3AAHF7AkgOzJgSdKbH1tuShLpdfaCWh4gzJhxemJEYkTKsc4VDh\n",
       "gvHQAu9IzWvUa0CLdH2IenAaRvSMJNslFNtsAAL4fFhRESbjg7zm8EPUBr3f2e95e3csnSoYBsYm\n",
       "hqSIst4Sys0geb7vWFVlCHIbazOPtrMVLfBjzmBlWtoc2pTIyjt2caqn4NsF/1kOi2kcL4MfeDFt\n",
       "oA3svufnO94tq+mxj3C3rvS9lxAgHAkAAAA7QZvISeEPJlMCTwAAAwAATZMCY04AIIBp2Z/i69xr\n",
       "o5c+hq1TQ9JOZ8jf7Q591U4300S7sUmC5f1gqYEAAABKQZ/mRRE8Jf8AACJMFBBFU0SGp6qQEdJQ\n",
       "vh8dxA/J2eR9SHct0+YjjvXc9Uqcxb+vshU3Hn+S1vozneyaePt5TjJwksxDz9PK0HEAAAAhAZ4F\n",
       "dELfAAARFfogftj3WSwOpopJF3oZvhZr2ymNANmBAAAAHAGeB2pC3wAAEUgEYbsWxxgHhZq9EMPG\n",
       "JByQAd0AAABBQZoJSahBaJlMCT8AAAMAAE5/xXz7InbhV+16fUPBGWSddyap5BNMNuo3aLSUTSHB\n",
       "H4LtYzkk4qie8CH0dxowYEAAAAA8QZotSeEKUmUwJP8AAAMAAE41FFdjRC6iLNxAJ0E3dSXCleIG\n",
       "U+C/O1NGdrEQwFDLgKjO/G76ndoyWQMDAAAAR0GeS0U0TCX/AAAiTBP1aucNoRIGEjlJwA+7MDbN\n",
       "TyEAAbiIJ37J3q8RJgMxhpDKH9TtmxUSQdLQCO3aazRru+02lQaq/AEHAAAAIAGeanRC3wAAERBk\n",
       "WRbAdVIKOEQllHjSE3wvLJ/LqAZ8AAAAHAGebGpC3wAAEUY3D3oBfbrmdr1G1qZgnND0AysAAAGO\n",
       "QZpvSahBaJlMFPJ/AAADAABNnDi2fvs8fy5A7V0sut/eTOYYZndtwKqYBpBrfaiKTNoDgJgfYJtt\n",
       "0kRRhVny1HZUCbA9VyFvUvRYm0PFdI2Yhj4+j0ssKm9W03V4QrMdQK/UZB34Cb/dd+62z/XZw4Jo\n",
       "ctsanab9AK2cKIpimIWBwUCa1XRh28LcJ/lTAxv9FN0RN7zB/oOJvSRy3GQBurUPgnSJyaHnk9BX\n",
       "hdqYdvGb8qxtC+ILHYLZtR0j0cgPKfbAgbVxJmwkTv4E5LK57d5wnJRMX3EMyRkjG0gLQOM3CD/E\n",
       "yP0Hey1bFBxumJ0bl/XsgOGfcKiH4fopisQbEOnNCH1KYcoCKBZM2vcKfDiwG/Ymffa4jIcWzyd/\n",
       "jq2fkCygK8KHkR1tu+Drienym7iEJIAC4ZzKOda+4KqmbRmrob3NxsidBL2M5GM86EcJMO/s+DRc\n",
       "vqdk1U9XvJTDEyJO9KXgYKgVbPsrsrq+kWapun2v+wyx+cUNilIQyjXwWNtPY/uSV84DJr0W98EA\n",
       "AAAwAZ6OakLfAAAmuZ+ftDrGj2sqkaxFvSQke0ZWlihk1KMqA74swfL8eJBuMY9Zy4PLAAAANUGa\n",
       "kEnhClJlMCT/AAADAABNktvLcgAFvFmWS0MCcaepXRLgxI0pcmmkKTmc6L3qConqAdTdAAABnkGa\n",
       "sUnhDomUwJP/AAADAABOf8WBSAZu5SrWCcxLzJtm6bIsppfU8TF36bKAh6h/U2CbnWRpjF8rjRix\n",
       "jkLqGyqj7RH8obi+L1d1HfeTSgNRXZgx9ByotDdnNqEZtGShozh45AQFCcyGOPj/3EfzH8TjJWXC\n",
       "exahKgI5BoUWAAkHweOEfkwSfK9Fyrxu58+IpzZXMmEyuF8tlSelBy1b0d3CAzE94/tyDNDt/g2P\n",
       "v7g5NmYQucor4W8SLqFqV3TGwaM/B/P+3mWkMVwd+aW7ImqZi+i0HCQzcheio8eYx8ejeEkvW10i\n",
       "qXX1/V/IOMt7gc/fW+ABwQV5QPWkF9W7WmhAc2NF9ycL7QS0PEGZMOL0xIjEix6SGc8fMOWDAUSN\n",
       "4hBBkfAy3R9iHpwGkb0jCTbJRTbZ8CL35gryTiQiCrk/wpim+3f2e95e3csnSnylsQXQuJbq+SCK\n",
       "jloGIqFyATxps+K6pnH21mKpEK37Vq8Uh2+7dOry33kNiQrPm2C/6yHRbSOF8GkQ1VVeA48Aztn9\n",
       "yki2r6bGPcLeljBgQAAAADdBmtRJ4Q8mUwJPAAADAABNkwJjTfIAAE/f5kCOlBTPuABjkRSB67Eh\n",
       "A1J2sXpnTlBEme6TkD0hAAAAOEGe8kURPCn/AAAlUYzZEUAbM2iEnIqABvZ4UnKpLw2bcAj673gA\n",
       "DyIYeEtaDi6VWEv9Q7DCGpvQAAAAFwGfE2pC3wAAEUgECQC0G+LSv2jYYAtoAAAAQEGbFUmoQWiZ\n",
       "TAk/AAADAABNnEjDUCgF8zdXQw/J7jLMCBFBTzvHYCr3bcEpOJ5oI4SMHDevtbeRxcG81V3IGBEA\n",
       "AAA5QZs3SeEKUmUwURLJ/wAAAwAATZMCXEPZ2bsAALFf1KUW1KWAJD9PkmRrS2jDQTFurWApTpqW\n",
       "AMGAAAAAJQGfVmpC3wAAJlSg3qkZlqZk9ebXw2KPWmNx+ettHfOwc7NxKbkAAAGRQZtaSeEOiZTA\n",
       "k/8AAAMAAE2S/gBa0jhO0to/WJmK8Q8emKNGBeFPqtYOpgGkLdNgJZM2gOAmB9gm23SRFGFWfKMf\n",
       "vl+oHquQt6l6LE2h4rpGzEKe9xL1lhU3q2lC+EKzHUCv1GQd+Am9pl6uttNnIVZ6EH2wkHp95jxW\n",
       "zhRFMUxCwOCb5Wq6MO3hbhP8qYGN/opuiJvRfT9BxZ6SOW4yAN1ah8GyU5uzc8noK8LtTDt4zfk7\n",
       "lOTUcU7BbNrimlwU76vDob3iJHdHOrFvLQd8Z4B2ae7zhOSiY+WJXSjJGNpAWgcZuEHtuzGBjZAl\n",
       "U12jdMTeRP+vZAcM+4VDRxx17mf9Ya557IRXEvwqqAjNv+NrgIxDMTvo0m9L7XEZC5iFW8Hv519A\n",
       "p7X+duK0JEdbbvg64np8pu4g+7NcQoS6D4TZ7uxBsslH8TFO5yYBL2M5GM86EcJSVTnGvaaBorfR\n",
       "HQoU+3u87sj2ug6CljzV7jdvpFmqbp9r/rXJThnDAQ4/gLCHWmtrOx/ckr5wGTXokOsAAAA5QZ94\n",
       "RRU8Kf8AACVRjNkRQJ9Rrp21/irz5oyvjpUSmmj4741INhUmsab/1rBj3jO50C7e6QU68cQoAAAA\n",
       "HgGfmWpC3wAAEUgD0yxOObwdR/GYZLjwvuPdPjFb2QAAADlBm5tJqEFomUwJPwAAAwAATZxG2Qll\n",
       "DO3x0g7amnuAtuOm+deOg4nnJXC7sGHxryceocSpTrYCMl0AAABIQZu8SeEKUmUwJP8AAAMAAE2S\n",
       "28t2Y4roVnLZ6Elf6iXlxEO552xX0AtNF0pTRkPmbhDz6ereNNDp7nrLHk2NaABWPN+YIeWhAAAA\n",
       "PUGbwEnhDomUwJP/AAADAABNktvLcgAFvsYll0AVpb5aFFRvE9EEopeW+1OYQMmetHUwKaX610Dn\n",
       "PNlFrZkAAAAtQZ/+RRE8Jf8AACJME/Vq5w0ottVdTuVMPeWBmHJXG862YoUuFThz5BOvWSDgAAAA\n",
       "HwGeHXRC3wAAERBkXfumGT5UE0TpzWg8JWfyE48ZuwYAAAAWAZ4fakLfAAARRjcQdrgsHJ6u4+sB\n",
       "bQAAAZ9BmgNJqEFomUwJPwAAAwAATZxJnYwMNcpVrBOYl5k2zdNrEFgyLxN+NbjlRETUP6mwTc6y\n",
       "NMYvlcaMWMchdQ2VUfaI/lDcXxeruo77yaUBqK7MGYY2n7Ubs5tQjNoyUNGcPHICAoTmQxx8f+4j\n",
       "+Y/icZMtjliWpSXCOQaFFgAJB8HjhH5MEnyvRcq8bufPiKc5BjGBNAOmuYlNErTfJVq1HBIxPeP7\n",
       "cgzQ7f4Nj7+4OucO58GSDjDwhzj/hvEW5GL6FU1vykoMwziuDvzS3ZE1TMX0Wu4SGbj7xVHjzGPj\n",
       "0bwkl62ukVS6+v6v5BxlvcDn73v5AUylXuYGwQ1q3a00IDmxovuThfaCWh4gzJhxemJEYkWHy5wq\n",
       "HDDlgwFEjeIQQZHwMt0fYh6cBpG9Iwk2yUU22fArXNlIoFee151yf4UxTfbv7Pe8vbuWTpT+S2IL\n",
       "oXEt1fJBFRy0DEVC5AJ402fFdUzj7azFUKPH9m3eKQ7fdunV5b7yGxIVnzbBf9ZDotpHC+DZoaqq\n",
       "vAceAZ2z+5SRbV9NjHuFvSxim+z0AAAAKkGeIUURLCn/AAAltqHZEyq4Z7iL0VQaHIfU8lPJNchB\n",
       "rGNiy9SJE28d0QAAACABnkJqQt8AABFIA9iYk7chsZrMIA2SeWfLNBocgrSLaAAAADlBmkdJqEFs\n",
       "mUwJPwAAAwAATZMCjSWh7UAAUa/qUstrv14Bc0SCC6LzbRY1aa3NmHsB53xLD4YIOmEAAAA0QZ5l\n",
       "RRUsJf8AACJME9yA9KeOZ0I4Fsdl8jEaVeUZW9xAB2EMDSOxk8ncZzi7dpGwkN2Z2wAAACABnoR0\n",
       "Qt8AABEV+X3Y1NuUnrH0cCXwzrzmrIF5WVxJwQAAABYBnoZqQt8AABFIBAiXF7BWwBqsEHNBAAAB\n",
       "l0GaiEmoQWyZTAk/AAADAABNnDi2fvs8fy5A7V0sut/eTOYYZndtwKqYBpBrfaiKTNoDgJgfYJtt\n",
       "0kRRjHI6y7vHZ1A9VyFvUvRYm0PFdI2Yhj4+j0ssKm9W03V4QrMdQK/UZB34Cb/dd+62z/XZw4Jo\n",
       "cuRZ+Sr+gFbOFEUxTELA4KBNarow7eFuE/qpgY3+im6Im95g/0HE3pI5bjIA3VqHwYJI5NDzyegr\n",
       "wu1MO3jN+VY2hfEFjsFs2pK4ICk1pPtgQNrNFuZ9TQ9uX3xngHZp7vOE5KKkIeqlCGO578OvACUT\n",
       "iIa3MYGNkCVTXaN0xOmLq49kBwz7hUQCwnXor1ioa435cAKoTlptBGbf8bXARl35nfRpN6X2uIyG\n",
       "10Kt4Pfvbbl85vVB5EOEN0lu+Drienym7iD5RUerBZ3oO5Hu7EGyyUfxMYqwP6bPVhA7c5i+hKfq\n",
       "c418Fx3ogK1PXf9i93ndkgScqQUmp1QxXV9Is1TdPtf+i5KcM4YCHH8BYR2oDbT2P7klfOAya9Fi\n",
       "cNEhdEFJpNmAAAAANEGaqUnhClJlMCT/AAADAABNktvLcgAFvAI5LQyZx56lcfAXWeDCVV61oqsv\n",
       "mtO/noFccZcAAABMQZrLSeEOiZTBTRM3AAADAACZyx2WDMz0V1InBzJ9qdm1z4TgGrm3Nzfwf1yG\n",
       "XQWbTLR4htEh5LUMU6hePrwDB1cpOVTR1U5cNQzYGQAAACgBnupqQt8AACZUoN6pGZKrHt6xwJfo\n",
       "F7y4GnVJPyfOFw4Wk26CC1KAAAABe0Ga7UnhDyZTBTzfAAADAAADAR792h2AHO5mHC8+9m7nsp+0\n",
       "FUN/0ULB0KzbPyda0M8zUqO9tsXtIdcPebr9RY8WsExpi0j+0mWK/648qWj4ui/DUrk6aLU5YQp7\n",
       "Oqs9gQimla6IE1DtmWqci0DXH8ZCN9zV4R5oDXWjGCPTcQlErrFDRx4dm1poRbhJWJzQWzGfrOxq\n",
       "BVGIRrlOI35yJWEibsm5lEjF74aRWdrEPNcrHvJKMIh30OjXMsgSB6RBa+3eYqBAtEI190L4VLFB\n",
       "hMNztn1zGhOftA2QKVAabEBiiALdAXoA/QDMwhCmMZ0scTtcWCFC1X/7Xm5GV9WXICC7T3y0g4Lu\n",
       "ijp+j1Tn2N8UJrx56/MOMNrtrXlXycXjLElbuKjq0lKZH72dsv9NAosEIBAs3aiJc9lzfJT44qkT\n",
       "ZUhWaJn2D+vSI0vOCJxbvEf4tBNmSHCgRNGq7IL5g6x4VNFySzB8VelzcYBuYDWeN4iCDwsAB94b\n",
       "6WgAAAAqAZ8MakLfAAAmVKDeqQAZoI1JQsoQBnu61XHm25Ju/Y/aHITk71UGOKLfAAAAOUGbEEnh\n",
       "DyZTAiP/AAADAAADASb9v0lGzuP9H+BRI0Adt+uEhZMPH0Zw2mxKJwP3YTM/ybOvjMaKkQAAACRB\n",
       "ny5FETwp/wAAJVGM2REvkJAAQ25orJ5AB0rYr2xJ12tWr28AAAAUAZ9PakLfAAARR/SNMv0Rzqqa\n",
       "P+kAAAGiQZtRSahBaJlMCI8AAAMAASbm0E668V8oMlmVL0I+NFV+VbLX+ukmp3Bvxk2jPxOYCiqT\n",
       "dSe5Hw3R++/et4qc/Ub2hQIhJOJO233Efusa5H9LXLnmEM0cZPPOGNkbWdrBOWcqva1OWjrna64j\n",
       "DZD1eMTTF1P4P7H7o8J5CES/h8OH8lhlbmFh5mhmFXt7cinn6id2pdjWA4+hR2p6Ok/3jnRqNZV2\n",
       "ops1hw5kWd4wHDuZa0pRIcDRsj1uy8GFuAy+4STPQs0gU+48A/2qN4rWlj9/LdSm8XuAgeQ3YEZ7\n",
       "5p4Lx2QguTOZjatHNODYRQoRFyxH+DJzaEqSfIt17YByepLsYTJ1jjrsfwk63v/RAYEiwZJAYfBC\n",
       "dqP9GAxLx6j7Mr2cNIXsOFevw26enTA6mEoDvOC/mwJaEJXR5U7K+ssjsk2lgzdbaIoVuuD6sEKK\n",
       "4hRKNkGjZ8AywEl63qbY4r3vmW+Kh25vHa/E3LFwvvF6qZiBrIkqYkI0uH5s7yU8X1NF48rKHdyH\n",
       "YEj5qEDpfwW2PaZX/gAPRS2vKswD2EZ6gAAAADZBm3JJ4QpSZTAiPwAAAwABI6BY1SuTzE8x7Lar\n",
       "LrtFDND5ZqpcD+vLA7wzmMPjM73znBCeg4EAAAA5QZuTSeEOiZTAiP8AAAMAASOgWOBuTr7P+i9A\n",
       "YXNwioyEjOpYPZz26y8IJTS7bkJDJBzEYuOwLJx1AAABnEGbtknhDyZTAiv/AAADAAKKWz83r7pg\n",
       "ZYfBr/MqXxxmxBeI1AH15d+dfPU9pAB/YA56IXW11TVuNdpvRbCdq3S2o+wJ1zcrvYkD/do3kfuZ\n",
       "EmYj9Cj5+X+sJ808OL0/Szyw9T3LW4Eiyu9naHWMwRsEN4d+k1XFnxFJGkXSQomTMSxnXSqyLfr/\n",
       "R9NUanZAIBLXT9uEAmcNqOMCPDTMGsVqN//wkm8rVQ6rJegRnzf24ck2luKXJem5t9dFhKS1twcc\n",
       "yPzyaoJTO8xF0+G8aiap8lzzDwaJgVAe9RG7ed2NO2Pie6GRQ5StOYyIt7gg9fjoACaDAhnpf7WL\n",
       "T2sMBIhop1oWk9nWo7WLde27nHgYLQjne/PnHmfiFgPTBYRLvmxo8DijfpSKMI8OaEYCSc8chHIH\n",
       "uphiS+/iRPyEUxC0pN9u/k17m9plTw4rP/dpXJ3VCOXpSLeFdcvvntqqyu/NbaaIvtq7/UCn9Ts/\n",
       "lzsFqkUlf/ck6M//WfOKQg9dQnPh58m0jeC4RreXRI3w/rUjtdNSQgFbvZ7IJeAAAAA1QZ/URRE8\n",
       "Kf8AACVRjNkRQRU9CmwcYtgIi509qA2lcIBc/D1Ty5uuDl5dP8wnS6TKUKL+iYEAAAAaAZ/1akLf\n",
       "AAARSAP8I04DIRO8qZSwQS9oFtAAAAA7QZv3SahBaJlMCM8AAAMAA4tv9ybPkli0KrnhYszk1rDi\n",
       "VQ7/uPp5/fiTzW8udwpKFQf4/YO1FJAwVsEAAAA6QZoYSeEKUmUwIz8AAAMAA4raZ7O/FRvbvI7a\n",
       "yAMVOR53QpgbsKCe/nuUQuPMj/K8JQAccDuDeUDAgQAAAZpBmjlJ4Q6JlMCO/wAAAwAFXh4ZGhIX\n",
       "ORkI91tOnKqemA7OerQNENiK8htia54osOZGsAxfAy4zOaK4PWXQUNyIvRdS+zOEhqvA61oYOiWs\n",
       "Iro4ToLXiKUFjOw4gBq7DSoUnWRF9GlJfJtahxVg1SYO7CmINYfrWxlE3kXQTpJV0lbgXrtByb4/\n",
       "vUrRMTx1v58BSCWqdiG8WlNr+OHDOV58a8z0UyCp7F8TzRYQudU9Y2fsZJO5aZS3lheHGQE3zhiD\n",
       "6vTvBrwAkkoMY0l2oe4Rxia133QrIaE/fDgwTMmpxdpZ1YQQ6DGgZ9y+1ACD1fGoejnpyTDXr8Cc\n",
       "9Nxj2+QWZiqQOpXXwLsSGnF8/Cgg3Ood5+3O8GrGl3oqKVZYslLWPPiGLnk5ABbVHKGqd2udqv4C\n",
       "oDQ+z1VfNVoIpUlBSZxF5CpkfrIL1wzONhy7SzPginbr+uhUuQplt4Ksx0IgjMO5Bx6lM5INZ5Sx\n",
       "/HTku3JWz0Z6wo4hnRxzQ0tl83EIiX/KIqvg4wxuNmjGz3voASRV1WARxsxl3AAAADNBmlpJ4Q8m\n",
       "UwIQ/wAAAwAHNqyx5JiH4AAmGLQH4AANQRvLHD+wBjrxvLmPpdEy/35gUkEAAAGCQZp7SeEPJlMC\n",
       "Ev8AAAMADNMjX1blf8Ijf71K2AOVCsO3EhQ8YHRC1Dj/ZrM9V8TwfyV+7CHmUq3dY0bsSlC9vgID\n",
       "3BOn6VChdfRNGlO7sWCvVEU1g4rKkOiVYkKdOZsuHcGqpYXD+Hg48c0NOUb6Wzv8vffCtLcQLcmL\n",
       "8/W1iI9kIroKGkVXi3v2aTvGK8Bizy6YCwD7KZHV3rpoaIV/vSeYwbcU/CXfoSCoF6cLNt+zrha+\n",
       "D4cFdogA4Y5DDXxMg2YB1G5UIt9OkrWGlXWDQF8KGZt9iRssD9RUmpr5wnQ/i+2vkOgH5hWDOitr\n",
       "gIiTZip16YHsvMqRAAaYMUU0ZB85YYBYGrioGtlW7WJwPhXqkcHfc0DP5LNu59VRWm8l3nB1gmgQ\n",
       "6/rjqTjsAHGuvjWVp6sFwzKxvWcEFZcubTV54vOwP8q8Xfu2VT9lTw7fx7ewIaWmXbJHnnTvj4f2\n",
       "3MYekTllVzLfXD4IjvpWb+jnXOrMlg1jUUZiMpIvKjWBma0AAAFlQZqcSeEPJlMCFv8AAAMAAxO3\n",
       "G5ZgEntCL+qVZ5Yr/jQWhz/fbq90QXTbV6FUu1+rQANuTev7vlIQTFW/GGrD2qebtAMbVjOQhCOJ\n",
       "KOVJCjieYwRym8QtG6UVmLcai9xasWR3G0qsu1Ocgni5X4WnFUMNs25YanyXqaxnTfQwo9br68p6\n",
       "7hsANvrAklCexB0ZbRrril0RB9OhgsUnmgevF4HLsU+vTajiMBzqB7fuCEVKMuh3Hhkv8dwztWO5\n",
       "aTzs9Y0fz3g7vOT1qAORbQgU20l/jvNGZrQHC19t8mh2r9d6dfky5ZZjfMH44j9SfczfzzyJgfMV\n",
       "wSO2H8v4NCnoBzt1pfQXkneDsjgM89Gk+xXntZ+NiuW1bdvdXj0Xc7kbHhYW5C4jCMC+gsNGHvuf\n",
       "n69heEYJQcsK+W5xV/VCKQ4Lc/oXdeRQ3Y0G+X2OLnQOp9zUxSiMGIEe4NuDz2tz4fzyRMxZAAAI\n",
       "N21vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAABhqAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAA\n",
       "AAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA\n",
       "AAdidHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAABhqAAAAAAAAAAAAAAAAAAAAAAAB\n",
       "AAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAH0AAAB9AAAAAAAJGVkdHMAAAAcZWxz\n",
       "dAAAAAAAAAABAAAYagAABAAAAQAAAAAG2m1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAKAAAAPoA\n",
       "VcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAABoVtaW5m\n",
       "AAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEA\n",
       "AAZFc3RibAAAALlzdHNkAAAAAAAAAAEAAACpYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAH0\n",
       "AfQASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADdh\n",
       "dmNDAWQAHv/hABpnZAAerNlAgBB554QAAAMABAAAAwCgPFi2WAEABmjr48siwP34+AAAAAAcdXVp\n",
       "ZGtoQPJfJE/FujmlG88DI/MAAAAAAAAAGHN0dHMAAAAAAAAAAQAAAH0AAAIAAAAAFHN0c3MAAAAA\n",
       "AAAAAQAAAAEAAAMgY3R0cwAAAAAAAABiAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAA\n",
       "AAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAwAABAAAAAABAAAGAAAA\n",
       "AAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAIAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAQAAAAA\n",
       "AQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAAB\n",
       "AAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEA\n",
       "AAAAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAAGAAAEAAAAAAEAAAYAAAAAAQAA\n",
       "AgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAACAAAAAACAAAC\n",
       "AAAAAAEAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIA\n",
       "AAAAAQAABgAAAAABAAACAAAAAAIAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAA\n",
       "AAABAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAA\n",
       "AAIAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAgAAAAA\n",
       "AgAAAgAAAAACAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAAC\n",
       "AAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAgAABAAAAAABAAAGAAAAAAEA\n",
       "AAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAgAAAAAAgAAAgAAAAADAAAEAAAAAAEAAAgAAAAAAgAA\n",
       "AgAAAAAGAAAEAAAAABxzdHNjAAAAAAAAAAEAAAABAAAAfQAAAAEAAAIIc3RzegAAAAAAAAAAAAAA\n",
       "fQAACLsAAAAeAAAAFQAAABMAAAATAAABfAAAACYAAAA5AAAAHwAAAFIAAAAzAAAARgAAAXMAAAAo\n",
       "AAAAOQAAACAAAABDAAABmwAAAYgAAAAvAAAAHQAAAX4AAABDAAAAKAAAAFMAAAA3AAAANwAAACEA\n",
       "AAAcAAAARAAAADsAAAAnAAAAHwAAABcAAAGZAAAARQAAACIAAAAfAAAAPgAAADwAAAAiAAAAOAAA\n",
       "ADUAAAA3AAABewAAADcAAAGPAAABogAAAC0AAABBAAAAJQAAAEcAAABLAAAAJQAAAYYAAAAyAAAA\n",
       "HQAAADsAAABAAAAAKgAAABoAAAG/AAAAMAAAACUAAAAdAAABmgAAACoAAAGoAAABpwAAAD8AAABO\n",
       "AAAAJQAAACAAAABFAAAAQAAAAEsAAAAkAAAAIAAAAZIAAAA0AAAAOQAAAaIAAAA7AAAAPAAAABsA\n",
       "AABEAAAAPQAAACkAAAGVAAAAPQAAACIAAAA9AAAATAAAAEEAAAAxAAAAIwAAABoAAAGjAAAALgAA\n",
       "ACQAAAA9AAAAOAAAACQAAAAaAAABmwAAADgAAABQAAAALAAAAX8AAAAuAAAAPQAAACgAAAAYAAAB\n",
       "pgAAADoAAAA9AAABoAAAADkAAAAeAAAAPwAAAD4AAAGeAAAANwAAAYYAAAFpAAAAFHN0Y28AAAAA\n",
       "AAAAAQAAADAAAABhdWR0YQAAAFltZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAA\n",
       "AAAAAAAAACxpbHN0AAAAJKl0b28AAAAcZGF0YQAAAAEAAAAATGF2ZjYxLjEuMTAw\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_agent(environment, policy):\n",
    "    frames = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "    while not done:\n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(range(4), 1, p=action_probs)\n",
    "        next_state, reward, done, extra_info = env.step(action)\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "        state = next_state\n",
    "\n",
    "    return display_video(frames)\n",
    "\n",
    "\n",
    "video = test_agent(env, random_policy)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca668f-4703-4a9a-8197-a4150025b134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-env",
   "language": "python",
   "name": "python-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
