{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415a380e-5c02-4014-929b-9d1694dbc48e",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization technique used to find the local minima of a multi-dimensional function, particularly when traditional methods like differentiation, polynomial division, or the pq formula are not applicable or difficult to solve.\n",
    "\n",
    "<ins>Sources</ins><br>\n",
    "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9<br>\n",
    "https://arxiv.org/abs/1802.01528"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c6d44-c102-4e79-a3ae-6898055da43c",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "<ins>MSE</ins><br>\n",
    "- stands for \"mean squared error\"\n",
    "- squares the differences between every output and it's true label & takes the average\n",
    "\n",
    "$C(y, o) = \\frac{1}{N} \\sum_{i=1}^N(y_{i} - o_{i})^2$\n",
    "- C: loss function\n",
    "- y: vector of true labels\n",
    "- o: vector of predictions\n",
    "\n",
    "<ins>MSE & Activation Function</ins><br>\n",
    "To determine the activation of a neuron, we begin by calculating the weighted sum of its inputs (wx) & adding the bias (b). This result is then passed through an activation function, which introduces non-linearity to the network.\n",
    "\n",
    "A widely used activation function is the ReLU (Rectified Linear Unit). ReLU outputs the input directly if it is positive; otherwise, it returns zero.\n",
    "\n",
    "$\\text{ReLU}(z) = \\max(0, z) = \\max(0, wx + b)$\n",
    "\n",
    "Now, we can substitute ReLU into the loss function.<br>\n",
    "First we have to substitute the predictions (o) with the weights (w) and inputs (X = collection of inputs) and biases of the loss function, then we can replace $O_{i}$ with ReLU.\n",
    "\n",
    "$C(y, w, X, b) = \\frac{1}{N} \\sum_{i=1}^N(y_{i} - \\max(0, wX_{i} + b))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd01eda-48fa-4975-aeda-a3e20729449f",
   "metadata": {},
   "source": [
    "## Slope\n",
    "\n",
    "The loss function's partial derivative with respect to the weights and biases is the slope we have to descent to optimize this parameters & minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caad0a5-4b60-46a7-b803-2064a0b3d8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
